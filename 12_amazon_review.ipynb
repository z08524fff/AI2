{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12.amazon_review",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Drd9KL8u6ut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2uqc-EEu9q0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import bz2\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "\n",
        "train_file = bz2.BZ2File('/content/drive/My Drive/train.ft.txt.bz2')\n",
        "test_file = bz2.BZ2File('/content/drive/My Drive/test.ft.txt.bz2')\n",
        "\n",
        "train_file = train_file.readlines()\n",
        "test_file = test_file.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBPc4TJxu9vQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_file[67])\n",
        "print(len(train_file))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1pO71WZu90B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_train = 800000  # We're training on the first 800,000 reviews in the dataset\n",
        "num_test = 200000  # Using 200,000 reviews from test set\n",
        "# num_train = len(train_file)\n",
        "# num_test = len(test_file)\n",
        "train_file = [x.decode('utf-8') for x in train_file[:num_train]]\n",
        "test_file = [x.decode('utf-8') for x in test_file[:num_test]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwXRAd7Ju92I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_file[67])\n",
        "print(len(train_file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWFzRAMwu94o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file]\n",
        "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file]\n",
        "\n",
        "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file]\n",
        "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_DPZ8Sku964",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(train_labels))\n",
        "print(train_labels[67])\n",
        "print(train_sentences[67])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQfEE3RHu99L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some simple cleaning of data\n",
        "for i in range(len(train_sentences)):\n",
        "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
        "\n",
        "for i in range(len(test_sentences)):\n",
        "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc_tOL5nu9_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_sentences[67])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFSILUTDu-B4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modify URLs to <url>\n",
        "for i in range(len(train_sentences)):\n",
        "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
        "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
        "        \n",
        "for i in range(len(test_sentences)):\n",
        "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
        "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExhrbmlmvOAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with_url = 0\n",
        "for ii, s in enumerate(train_sentences):\n",
        "    if '<url>' in s:\n",
        "        with_url = ii\n",
        "        print(ii)\n",
        "        break\n",
        "print(train_file[5])\n",
        "print(train_sentences[with_url])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybhGsEdsvOCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = Counter()  # Dictionary that will map a word to the number of times it appeared in all the training sentences\n",
        "for i, sentence in enumerate(train_sentences):\n",
        "    # The sentences will be stored as a list of words/tokens\n",
        "    train_sentences[i] = []\n",
        "    for word in nltk.word_tokenize(sentence):  # Tokenizing the words\n",
        "        words.update([word.lower()])  # Converting all the words to lowercase\n",
        "        train_sentences[i].append(word)\n",
        "    if i%20000 == 0:\n",
        "        print(str((i*100)/num_train) + \"% done\")\n",
        "print(\"100% done\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc73HZkivOFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_sentences[67])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMowf9LyvOJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = {k:v for k,v in words.items() if v>1}\n",
        "# Sorting the words according to the number of appearances, with the most common word being first\n",
        "words = sorted(words, key=words.get, reverse=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKH67wG3vOLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(words))\n",
        "print(words[100])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9GBdZ5IvONn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = ['_PAD','_UNK'] + words\n",
        "# Dictionaries to store the word to index mappings and vice versa\n",
        "word2idx = {o:i for i,o in enumerate(words)}\n",
        "idx2word = {i:o for i,o in enumerate(words)}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuQXC3_mvXKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(word2idx['the'])\n",
        "print(idx2word[100])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb68kltEvXMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, sentence in enumerate(train_sentences):\n",
        "    # Looking up the mapping dictionary and assigning the index to the respective words\n",
        "    train_sentences[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n",
        "\n",
        "for i, sentence in enumerate(test_sentences):\n",
        "    # For test sentences, we have to tokenize the sentences as well\n",
        "    test_sentences[i] = [word2idx[word.lower()] if word.lower() in word2idx else 0 for word in nltk.word_tokenize(sentence)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFSXgqDNvXOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(train_sentences[67]))\n",
        "print(train_sentences[67])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z6eBEAUvXRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "more_than_200 = 0\n",
        "for ii, s in enumerate(train_sentences):\n",
        "    if len(s) > 200:\n",
        "        more_than_200 = ii\n",
        "        print(len(s))\n",
        "        print(ii)\n",
        "        break\n",
        "print(train_file[more_than_200])\n",
        "print(train_sentences[more_than_200])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPVA2m3kvXTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\n",
        "def pad_input(sentences, seq_len):\n",
        "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
        "    for ii, review in enumerate(sentences):\n",
        "        if len(review) != 0:\n",
        "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "    return features\n",
        "\n",
        "seq_len = 200  # The length that the sentences will be padded/shortened to\n",
        "\n",
        "train_sentences = pad_input(train_sentences, seq_len)\n",
        "test_sentences = pad_input(test_sentences, seq_len)\n",
        "\n",
        "# Converting our labels into numpy arrays\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grRcuoWCvXVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(train_sentences[67]))\n",
        "print(train_sentences[67])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J509Jf4RveZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(len(train_sentences[more_than_200]))\n",
        "print(train_sentences[more_than_200])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT8EuX1XvecQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_frac = 0.5 # 50% validation, 50% test\n",
        "split_id = int(split_frac * len(test_sentences))\n",
        "val_sentences, test_sentences = test_sentences[:split_id], test_sentences[split_id:]\n",
        "val_labels, test_labels = test_labels[:split_id], test_labels[split_id:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH8F2omEveeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(val_sentences))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSQ6LQjbvegY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
        "val_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(val_labels))\n",
        "test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
        "\n",
        "batch_size = 400\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B75bnLbqveij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REy-bdElvmBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentimentNet(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        super(SentimentNet, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        out = self.sigmoid(out)\n",
        "        \n",
        "        out = out.view(batch_size, -1)\n",
        "        out = out[:,-1]\n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e81Omg7vmD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(word2idx) + 1\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "\n",
        "model = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "model.to(device)\n",
        "\n",
        "lr=0.005\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l9t2Zm6vmGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hh = model.init_hidden(batch_size)\n",
        "hh = tuple([e.data for e in hh])\n",
        "print(hh[1].shape)\n",
        "print(hh[1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAhM8DMZvmIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = next(iter(train_loader)) # 取一段training data出來\n",
        "x = inputs[0].to(device) #把資料移入gpu\n",
        "batch_num = x.size(0) #取得批次大小，256\n",
        "x = x.long() # 把輸入值轉成長整數\n",
        "print(x.shape) # 看看x的型狀長什麼樣子\n",
        "print(x) #印出一個x來看看\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPSp7tY2vek4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding = nn.Embedding(vocab_size, embedding_dim) # 把x從one-hot的詞彙大小轉成word2vec的大小\n",
        "embedding = embedding.to(device) #將這個函數放入gpu\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8fsSKSmvu7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeds = embedding(x) # 將詞彙轉成wordvec\n",
        "embeds.shape #看看型狀\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqjzaIv3vu9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=0.5, batch_first=True) # 建立一個lstm\n",
        "lstm = lstm.to(device) #將lstm移入gpu\n",
        "lstm_out, hidden = lstm(embeds, hh) #讓剛才的資料跑過lstm，產生輸出值及新的hidden值\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSL5c3Dovu_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(lstm_out.shape) #看看從lstm跑出來的東西型狀\n",
        "print(hidden[0].shape) # 看看hidden第一個值的形狀\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Bgyw1A9vvCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_out = lstm_out.contiguous().view(-1, hidden_dim) #把lstm的輸出值攤平\n",
        "lstm_out.shape #看看形狀\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87gg9BlpvvEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dropout = nn.Dropout(0.5) #設定dropout\n",
        "out = dropout(lstm_out) # 讓輸出值走一次dropout\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNQmsZe3vvHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(out.shape) # 查看輸出值的形狀"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viGRkR6VvvLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fc = nn.Linear(hidden_dim, 1) #定義最後一層fc\n",
        "fc = fc.to(device) #移入gpu\n",
        "out = fc(out) #讓輸入出值走一次fc\n",
        "print(out.shape) # 看一下形狀\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaHIviszvvOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sigmoid = nn.Sigmoid() # 讓輸出值介於0和1之間\n",
        "sigmoid = sigmoid.to(device) #將sigmoid移入gpu\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zVSDe5-vvV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out = sigmoid(out) #讓輸出值走一次sigmoid\n",
        "print(out.shape) #看看形狀\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhG_EA5wv7RO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out = out.view(batch_size, -1) #把輸出值還原成(批次大小，句子長度)\n",
        "print(out.shape) # 看形狀\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_4DvLZ7v7Ti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out = out[:,-1] #取出最後結果值\n",
        "print(out.shape)# 看形狀\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVk1A_8Hv7Vz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(out) # 看每批256個輸出值\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMfDsUR6v7YB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 2\n",
        "counter = 0\n",
        "print_every = 1000\n",
        "clip = 5\n",
        "valid_loss_min = np.Inf\n",
        "\n",
        "model.train()\n",
        "for i in range(epochs):\n",
        "    h = model.init_hidden(batch_size)\n",
        "    \n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "        h = tuple([e.data for e in h])\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        model.zero_grad()\n",
        "        output, h = model(inputs, h)\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        \n",
        "        if counter%print_every == 0:\n",
        "            val_h = model.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "            for inp, lab in val_loader:\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "                inp, lab = inp.to(device), lab.to(device)\n",
        "                out, val_h = model(inp, val_h)\n",
        "                val_loss = criterion(out.squeeze(), lab.float())\n",
        "                val_losses.append(val_loss.item())\n",
        "                \n",
        "            model.train()\n",
        "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "            if np.mean(val_losses) <= valid_loss_min:\n",
        "                torch.save(model.state_dict(), './state_dict.pt')\n",
        "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
        "                valid_loss_min = np.mean(val_losses)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKyE4Dyzv7aj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the best model\n",
        "model.load_state_dict(torch.load('./state_dict.pt'))\n",
        "\n",
        "test_losses = []\n",
        "num_correct = 0\n",
        "h = model.init_hidden(batch_size)\n",
        "\n",
        "model.eval()\n",
        "for inputs, labels in test_loader:\n",
        "    h = tuple([each.data for each in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output, h = model(inputs, h)\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    pred = torch.round(output.squeeze())  # Rounds the output to 0/1\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}